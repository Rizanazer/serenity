const [ToxicCheckmessage, setToxicCheckMessage] = useState("");
const [toxicScore, setToxicScore] = useState(null);
const [insultScore, setInsultScore] = useState(null);
const [obsceneScore, setObsceneScore] = useState(null);
const [identityHateScore, setIdentityHateScore] = useState(null);
const [threatScore, setThreatScore] = useState(null);
const [severeToxicScore, setSevereToxicScore] = useState(null);
 //////////////////////////////////////commentfilter///////////////////////////////////////////
  async function query(data) {
    const response = await fetch(
      "https://api-inference.huggingface.co/models/unitary/toxic-bert",
      {
        headers: { Authorization: "Bearer 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" },
        method: "POST",
        body: JSON.stringify(data),
      }
    );
    const result = await response.json();
    return result;
  }

  query({ "inputs": ToxicCheckmessage }).then((response) => {

    const toxicScore = response[0][0].score;
    const insultScore = response[0][1].score;
    const obsceneScore = response[0][2].score;
    const identityHateScore = response[0][3].score;
    const threatScore = response[0][4].score;
    const severeToxicScore = response[0][5].score;

    setToxicScore(toxicScore);
    setInsultScore(insultScore);
    setObsceneScore(obsceneScore);
    setIdentityHateScore(identityHateScore);
    setThreatScore(threatScore);
    setSevereToxicScore(severeToxicScore);


  });
  /////////////////////////////////////////////////////////////////////////////////
  ////////////////////////////messageforward////////////////////////////////////////
  const toxicityThreshold = 0.5;
  if (toxicScore > toxicityThreshold || insultScore > toxicityThreshold || obsceneScore > toxicityThreshold ||
    identityHateScore > toxicityThreshold || threatScore > toxicityThreshold || severeToxicScore > toxicityThreshold) {
    // Message is toxic
    console.log('Message is toxic');
  } else {
   send()
  }